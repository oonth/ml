Program 1

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv(r'C:\Users\aishwarya akshaya\Desktop\notes\ml\housing.csv')
df.info()
df.nunique()
df.isnull().sum()
df.duplicated().sum()
df['total_bedrooms'].median()
df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)
for i in df.iloc[:,2:7]:
    df[i] = df[i].astype('int')
    df.head()
    df.describe().T
Numerical = df.select_dtypes(include=[np.number]).columnsprint(Numerical)
for col in Numerical:
    plt.figure(figsize=(10, 6))
    df[col].plot(kind='hist', title=col, bins=60, edgecolor='black')
    plt.ylabel('Frequency')
    plt.show()
for col in Numerical:
     plt.figure(figsize=(6, 6))
     sns.boxplot(df[col], color='blue')
     plt.title(col)
     plt.ylabel(col)
plt.show()


Program 2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import ssl
ssl._create_default_https_context=ssl._create_unverified_context
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['Target'] = data.target
variable_meaning = {
 "MedInc": "Median income in block group",
 "HouseAge": "Median house age in block group",
 "AveRooms": "Average number of rooms per household",
 "AveBedrms": "Average number of bedrooms per household",
 "Population": "Population of block group",
 "AveOccup": "Average number of household members",
 "Latitude": "Latitude of block group",
 "Longitude": "Longitude of block group",
 "Target": "Median house value (in $100,000s)"
}
variable_df = pd.DataFrame(list(variable_meaning.items()), columns=["Feature", "Description"])
print ("\nVariable Meaning Table:")
print(variable_df)
print ("\nBasic Information about Dataset:")
print (df.info ()) 
print ("\nFirst Five Rows of Dataset:")
print(df.head()) 
print ("\nSummary Statistics:")
print(df.describe())
print ("\nMissing Values in Each Column:")
print(df.isnull().sum()) 
plt.figure(figsize=(12, 8))
df.hist(figsize=(12, 8), bins=30, edgecolor='black')
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()
plt.figure(figsize=(12, 6))
sns.boxplot(data=df)
plt.xticks(rotation=45)
plt.title("Boxplots of Features to Identify Outliers")
plt.show()
plt.figure(figsize=(10, 6))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()
sns.pairplot(df[['MedInc', 'HouseAge', 'AveRooms', 'Target']], diag_kind='kde')
plt.show()
print ("\nKey Insights:")
print ("1. The dataset has", df.shape[0], "rows and", df.shape[1], "columns.")
print ("2. No missing values were found in the dataset.")
print ("3. Histograms show skewed distributions in some features like 'MedInc'.")
print ("4. Boxplots indicate potential outliers in 'AveRooms' and 'AveOccup'.")
print ("5. Correlation heatmap shows 'MedInc' has the highest correlation with house prices.")



Program 3

 import numpy as np
 import pandas as pd
 from sklearn.datasets import load_iris
 from sklearn.decomposition import PCA
 import matplotlib.pyplot as plt
 iris = load_iris()
 data = iris.data
 labels = iris.target
 label_names = iris.target_names
 iris_df = pd.DataFrame(data, columns=iris.feature_names)
 pca = PCA(n_components=2)
 data_reduced = pca.fit_transform(data)
 reduced_df = pd.DataFrame(data_reduced, columns=['Principal Component 1', 'Principal Component 2'])
 reduced_df['Label'] = labels
 plt.figure(figsize=(8, 6))
 colors = ['r', 'g', 'b']
 for i, label in enumerate(np.unique(labels)):
    plt.scatter(
        reduced_df[reduced_df['Label'] == label]['Principal Component 1'],
        reduced_df[reduced_df['Label'] == label]['Principal Component 2'],
        label=label_names[label],
        color=colors[i]
    )
 plt.title('PCA on Iris Dataset')
 plt.xlabel('Principal Component 1')
 plt.ylabel('Principal Component 2')
 plt.legend()
 plt.grid()
 plt.show()

Program 4


 import csv
 a = []
 with open(r"C:\Users\aishwarya akshaya\Downloads\enjoysport.csv", 'r') as csvfile:
    for row in csv.reader(csvfile):
        a.append(row)
    print(a)
 print("\n The total number of training instances are : ",len(a))
 num_attribute = len(a[0])-1
 print("\n The initial hypothesis is : ")
 hypothesis = ['0']*num_attribute
 print(hypothesis)
 for i in range(0, len(a)):
    if a[i][num_attribute] == 'yes':
        for j in range(0, num_attribute):
            if hypothesis[j] == '0' or hypothesis[j] == a[i][j]:
                hypothesis[j] = a[i][j]
            else:
                hypothesis[j] = '?'
    print("\n The hypothesis for the training instance {} is : \n" .format(i+1),hypothesis)
 print("\n The Maximally specific hypothesis for the training instance is ")
 print(hypothesis)





Program 5



 import numpy as np
 import matplotlib.pyplot as plt
 from collections import Counter
 data = np.random.rand(100)
 labels = ["Class1" if x <= 0.5 else "Class2" for x in data[:50]]
 def euclidean_distance(x1, x2):
    return abs(x1 - x2)
 def knn_classifier(train_data, train_labels, test_point, k):
    distances = [(euclidean_distance(test_point, train_data[i]), train_labels[i]) for i in 
range(len(train_data))]
    distances.sort(key=lambda x: x[0])
    k_nearest_neighbors = distances[:k]
    k_nearest_labels = [label for _, label in k_nearest_neighbors]
    return Counter(k_nearest_labels).most_common(1)[0][0]
 train_data = data[:50]
 train_labels = labels
 test_data = data[50:]
 k_values = [1, 2, 3, 4, 5, 20, 30]
 print("--- k-Nearest Neighbors Classification ---")
 print("Training dataset: First 50 points labeled based on the rule (x <= 0.5 -> Class1, x > 0.5 -> Class2)")
 print("Testing dataset: Remaining 50 points to be classified\n")
 results = {}
 for k in k_values:
    print(f"Results for k = {k}:")
    classified_labels = [knn_classifier(train_data, train_labels, test_point, k) for test_point in test_data]
    results[k] = classified_labels
    for i, label in enumerate(classified_labels, start=51):
        print(f"Point x{i} (value: {test_data[i - 51]:.4f}) is classified as {label}")
    print("\n")
 print("Classification complete.\n")
 for k in k_values:
    classified_labels = results[k]
    class1_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == "Class1"]
    class2_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == "Class2"]
    plt.figure(figsize=(10, 6))
    plt.scatter(train_data, [0] * len(train_data), c=["blue" if label == "Class1" else "red" for label in train_labels],label="Training Data", marker="o")
    plt.scatter(class1_points, [1] * len(class1_points), c="blue", label="Class1 (Test)", marker="x")
    plt.scatter(class2_points, [1] * len(class2_points), c="red", label="Class2 (Test)", marker="x")
    plt.title(f"k-NN Classification Results for k = {k}")
    plt.xlabel("Data Points")
    plt.ylabel("Classification Level")
    plt.legend()
    plt.grid(True)
    plt.show()


Program 6


 import numpy as np
 import matplotlib.pyplot as plt
 def gaussian_kernel(x, xi, tau):
    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))
 def locally_weighted_regression(x, X, y, tau):
    m = X.shape[0]
    weights = np.array([gaussian_kernel(x, X[i], tau) for i in range(m)])
    W = np.diag(weights)
    X_transpose_W = X.T @ W
    theta = np.linalg.inv(X_transpose_W @ X) @ X_transpose_W @ y
    return x @ theta
 np.random.seed(42)
 X = np.linspace(0, 2 * np.pi, 100)
 y = np.sin(X) + 0.1 * np.random.randn(100)
 X_bias = np.c_[np.ones(X.shape), X]
 x_test = np.linspace(0, 2 * np.pi, 200)
 x_test_bias = np.c_[np.ones(x_test.shape), x_test]
 tau = 0.5
 y_pred = np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in x_test_bias])
 plt.figure(figsize=(10, 6))
 plt.scatter(X, y, color='red', label='Training Data', alpha=0.7)
 plt.plot(x_test, y_pred, color='blue', label=f'LWR Fit (tau={tau})', linewidth=2)
 plt.xlabel('X', fontsize=12)
 plt.ylabel('y', fontsize=12)
 plt.title('Locally Weighted Regression', fontsize=14)
 plt.legend(fontsize=10)
 plt.grid(alpha=0.3)
 plt.show()



Program 7
a


 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 import ssl
 ssl._create_default_https_context = ssl._create_unverified_context
 from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error, r2_score
 from sklearn.preprocessing import StandardScaler
 import warnings
 warnings.filterwarnings('ignore')
 data = pd.read_csv(r"C:\Users\aishwarya akshaya\Downloads\archive\HousingData.csv")
 data.head()
 data.shape
 data.info()
 data.nunique()
 data.CHAS.unique()
 data.ZN.unique()
 data.isnull().sum()
 data.duplicated().sum()
 df = data.copy()
 df['CRIM'].fillna(df['CRIM'].mean(), inplace=True)
 df['ZN'].fillna(df['ZN'].mean(), inplace=True)
 df['CHAS'].fillna(df['CHAS'].mode()[0], inplace=True)
 df['INDUS'].fillna(df['INDUS'].mean(), inplace=True)
 df['AGE'].fillna(df['AGE'].median(), inplace=True)  
 df['LSTAT'].fillna(df['LSTAT'].median(), inplace=True)
 df.isnull().sum()
 df.head()
 df['CHAS'] = df['CHAS'].astype('int')
 df.describe().T
 for i in df.columns:
    plt.figure(figsize=(6,3))
    plt.subplot(1, 2, 1)
    df[i].hist(bins=20, alpha=0.5, color='b',edgecolor='black')
    plt.title(f'Histogram of {i}')
    plt.xlabel(i)
    plt.ylabel('Frequency')
    plt.subplot(1, 2, 2)
    plt.boxplot(df[i], vert=False)
    plt.title(f'Boxplot of {i}')
    plt.show()
    corr = df.corr(method='pearson')
 plt.figure(figsize=(10, 8))
 sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
 plt.xticks(rotation=90, ha='right')
 plt.yticks(rotation=0)
 plt.title("Correlation Matrix Heatmap")
 plt.show()
 X = df.drop('MEDV', axis=1)  
 y = df['MEDV']  
 scale = StandardScaler()
 X_scaled  = scale.fit_transform(X)
 X_train, X_test, y_train, y_test = train_test_split(X_scaled , y, test_size=0.2, random_state=42)
 model = LinearRegression()
 model.fit(X_train, y_train)
 y_pred = model.predict(X_test)
 y_pred
 mse = mean_squared_error(y_test, y_pred)
 rmse = np.sqrt(mse)
 r2 = r2_score(y_test, y_pred)
 print(f'Mean Squared Error: {mse}')
 print(f'Root Mean Squared Error: {rmse}')
 print(f'R-squared: {r2}')


Program 7
b
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 import ssl
 ssl._create_default_https_context = ssl._create_unverified_context
 from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error, r2_score
 from sklearn.preprocessing import StandardScaler
 import warnings
 warnings.filterwarnings('ignore')
 data = pd.read_csv(r"C:\Users\aishwarya akshaya\Downloads\archive\HousingData.csv")
 data.head()
 data.shape
 data.info()
 data.nunique()
 data.CHAS.unique()
 data.ZN.unique()
 data.isnull().sum()
 data.duplicated().sum()
 df = data.copy()
 df['CRIM'].fillna(df['CRIM'].mean(), inplace=True)
 df['ZN'].fillna(df['ZN'].mean(), inplace=True)
 df['CHAS'].fillna(df['CHAS'].mode()[0], inplace=True)
 df['INDUS'].fillna(df['INDUS'].mean(), inplace=True)
 df['AGE'].fillna(df['AGE'].median(), inplace=True)  
 df['LSTAT'].fillna(df['LSTAT'].median(), inplace=True)
 df.isnull().sum()
 df.head()
 df['CHAS'] = df['CHAS'].astype('int')
 df.describe().T
 for i in df.columns:
    plt.figure(figsize=(6,3))
    plt.subplot(1, 2, 1)
    df[i].hist(bins=20, alpha=0.5, color='b',edgecolor='black')
    plt.title(f'Histogram of {i}')
    plt.xlabel(i)
    plt.ylabel('Frequency')
    plt.subplot(1, 2, 2)
    plt.boxplot(df[i], vert=False)
    plt.title(f'Boxplot of {i}')
    plt.show()
    corr = df.corr(method='pearson')
 plt.figure(figsize=(10, 8))
 sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
 plt.xticks(rotation=90, ha='right')
 plt.yticks(rotation=0)
 plt.title("Correlation Matrix Heatmap")
 plt.show()
 X = df.drop('MEDV', axis=1)  
 y = df['MEDV']  
 scale = StandardScaler()
 X_scaled  = scale.fit_transform(X)
 X_train, X_test, y_train, y_test = train_test_split(X_scaled , y, test_size=0.2, random_state=42)
 model = LinearRegression()
 model.fit(X_train, y_train)
 y_pred = model.predict(X_test)
 y_pred
 mse = mean_squared_error(y_test, y_pred)
 rmse = np.sqrt(mse)
 r2 = r2_score(y_test, y_pred)
 print(f'Mean Squared Error: {mse}')
 print(f'Root Mean Squared Error: {rmse}')
 print(f'R-squared: {r2}')



Program 8



import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree

data = load_breast_cancer()
X = data.data
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print (f"Model Accuracy: {accuracy * 100:.2f} %")
new_sample = np.array([X_test[0]])
prediction = clf.predict(new_sample)
prediction_class = "Benign" if prediction == 1 else "Malignant"
print (f"Predicted Class for the new sample: {prediction_class}")
plt.figure(figsize=(12,8))
tree.plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names)
plt.title("Decision Tree - Breast Cancer Dataset")
plt.show()



Program 9



 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.datasets import fetch_olivetti_faces
 data = fetch_olivetti_faces()
 data.keys()
 print ("Data Shape:", data.data.shape)
 print ("Target Shape:", data.target.shape)
 print ("There are {} unique persons in the dataset".format(len(np.unique(data.target))))
 print ("Size of each image is {}x{}". format(data.images.shape[1],data.images.shape[1]))
 def print_faces(images, target, top_n):
    top_n = min (top_n, len(images))
    grid_size = int(np.ceil(np.sqrt(top_n)))
    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)
    for i, ax in enumerate(axes.ravel()):
        if i < top_n:
            ax.imshow(images[i], cmap='bone')
            ax.axis('off')
            ax.text(2, 12, str(target[i]), fontsize=9, color='red')
            ax.text(2, 55, f"face: {i}", fontsize=9, color='blue')
        else:
            ax.axis('off')
    plt.show()
 print_faces(data. images, data. target,400)
 def display_unique_faces(pics):
    fig = plt.figure(figsize=(24, 10)) 
    columns, rows = 10, 4 
    for i in range(1, columns * rows + 1):
        img_index = 10 * i - 1 
        if img_index < pics.shape[0]: 
            img = pics[img_index, :, :]
            ax = fig.add_subplot(rows, columns, i)
            ax.imshow(img, cmap='gray')
            ax.set_title(f"Person {i}", fontsize=14)
            ax.axis('off')
    plt.suptitle("There are 40 distinct persons in the dataset", fontsize=24)
    plt.show()
 display_unique_faces(data.images)
 from sklearn.model_selection import train_test_split
 X = data.data
 Y = data.target
 x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=46)
 print("x_train: ",x_train.shape)
 print("x_test: ",x_test.shape)
 from sklearn.naive_bayes import GaussianNB
 from sklearn.metrics import confusion_matrix, accuracy_score
 nb = GaussianNB()
 nb.fit(x_train, y_train)
 y_pred = nb.predict(x_test)
 nb_accuracy = round (accuracy_score(y_test, y_pred) * 100, 2)
 cm = confusion_matrix(y_test, y_pred)
 print ("Confusion Matrix:")
 print(cm)
 print (f"Naive Bayes Accuracy: {nb_accuracy}%")
 from sklearn.naive_bayes import MultinomialNB
 from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
 nb = MultinomialNB()
 nb.fit(x_train, y_train)
 y_pred = nb.predict(x_test)
 accuracy = round (accuracy_score(y_test, y_pred) * 100, 2)
 print (f"Multinomial Naive Bayes Accuracy: {accuracy}%")
 misclassified_idx = np.where(y_pred != y_test)[0]
 num_misclassified = len(misclassified_idx)
 print (f"Number of misclassified images: {num_misclassified}")
 print (f"Total images in test set: {len(y_test)}")
 print (f"Accuracy: {round ((1 - num_misclassified / len(y_test)) * 100, 2)} %")
 n_misclassified_to_show = min (num_misclassified, 5) # Show up to 5 misclassified images
 plt.figure(figsize=(10, 5))
 for i in range(n_misclassified_to_show):
    idx = misclassified_idx[i]
    plt.subplot(1, n_misclassified_to_show, i + 1)
    plt.imshow(x_test[idx].reshape(64, 64), cmap='gray')
    plt.title(f"True: {y_test[idx]}, Pred: {y_pred[idx]}")
    plt.axis('off')
 plt.show()
 from sklearn.preprocessing import label_binarize
 from sklearn.metrics import roc_auc_score
 y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
 y_pred_prob = nb.predict_proba(x_test)
 for i in range(y_test_bin.shape[1]):
    roc_auc = roc_auc_score(y_test_bin[:, i], y_pred_prob[:, i])
    print (f"Class {i} AUC: {roc_auc:.2f}")



Program 10


 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 import seaborn as sns
 from sklearn.datasets import load_breast_cancer
 from sklearn.cluster import KMeans
 from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
 from sklearn.metrics import confusion_matrix, classification_report
 data = load_breast_cancer()
 X = data.data
 y = data.target
 scaler = StandardScaler()
 X_scaled = scaler.fit_transform(X)
 kmeans = KMeans(n_clusters=2, random_state=42)
 y_kmeans = kmeans.fit_predict(X_scaled)
 print ("Confusion Matrix:")
 print (confusion_matrix(y, y_kmeans))
 print ("\nClassification Report:")
 print (classification_report(y, y_kmeans))
 pca = PCA(n_components=2)
 X_pca = pca.fit_transform(X_scaled)
 df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
 df['Cluster'] = y_kmeans
 df['True Label'] = y
 plt.figure(figsize=(8, 6))
 sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black', alpha=0.7)
 plt.title('K-Means Clustering of Breast Cancer Dataset')
 plt.xlabel('Principal Component 1')
 plt.ylabel('Principal Component 2')
 plt.legend(title="Cluster")
 plt.show()
 plt.figure(figsize=(8, 6))
 sns.scatterplot(data=df, x='PC1', y='PC2', hue='True Label', palette='coolwarm', s=100,  edgecolor='black', alpha=0.7)
 plt.title('True Labels of Breast Cancer Dataset')
 plt.xlabel('Principal Component 1')
 plt.ylabel('Principal Component 2')
 plt.legend(title="True Label")
 plt.show()
 plt.figure(figsize=(8, 6))
 sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black', alpha=0.7)
 centers = pca.transform(kmeans.cluster_centers_)
 plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')
 plt.title('K-Means Clustering with Centroids')
 plt.xlabel('Principal Component 1')
 plt.ylabel('Principal Component 2')
 plt.legend(title="Cluster")
 plt.show()
